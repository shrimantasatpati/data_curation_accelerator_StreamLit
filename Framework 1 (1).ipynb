{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53267d50-3797-4198-82e5-6bcb7c6b3911",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkFiles,SparkContext,SparkConf\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62a497cf-a435-4e7b-b87d-d8a6694bde22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_uppercase(df):\n",
    "    uppercase = [i for i in range(len(df.columns)) if df.columns[i] != df.columns[i].lower()]\n",
    "    return uppercase\n",
    "\n",
    "def check_space(df):\n",
    "    columns_with_space = [i for i in range(len(df.columns)) if df.columns[i] != \"\".join(df.columns[i].split())]\n",
    "    return columns_with_space\n",
    "\n",
    "def check_for_special_characters(df):\n",
    "    special_col = [i for i in range(len(df.columns)) if re.search(r\"[^a-zA-z_ 0-9]\", df.columns[i])]\n",
    "    return special_col\n",
    "\n",
    "def implement_header_validation(df):\n",
    "    output = {}\n",
    "    uppercase_columns = check_uppercase(df)\n",
    "    if len(uppercase_columns) > 0:\n",
    "        output[\"Have Uppercase\"] = uppercase_columns\n",
    "\n",
    "    columns_with_space = check_space(df)\n",
    "    if len(columns_with_space) > 0:\n",
    "        output[\"Have Space\"] = columns_with_space\n",
    "\n",
    "    special_columns = check_for_special_characters(df)\n",
    "    if len(special_columns) > 0:\n",
    "        output[\"Have Special Characters\"] = special_columns\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cd02920-81d6-439d-8c9f-b3755ce8d1c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import datetime\n",
    "\n",
    "\n",
    "def header_small(uppercase_columns, df):\n",
    "    for i in uppercase_columns:\n",
    "        df = df.withColumnRenamed(df.columns[i], df.columns[i].lower())\n",
    "    return df\n",
    "\n",
    "def remove_spaces_add_underscore(columns_with_space, df):\n",
    "    for i in columns_with_space:\n",
    "        df = df.withColumnRenamed(df.columns[i], \"_\".join(df.columns[i].split()))\n",
    "    return df\n",
    "\n",
    "def remove_special_characters(special_col, df):\n",
    "    for i in special_col:\n",
    "        new = re.sub(\"[^a-zA-Z_0-9]\", \"\", df.columns[i])\n",
    "        df = df.withColumnRenamed(df.columns[i], new)\n",
    "    return df\n",
    "\n",
    "def change_header(df, count1=None):\n",
    "    h_none = []\n",
    "    dummy = []\n",
    "    for h, i in enumerate(df.columns):\n",
    "        if re.match(r\"^_c\\d{1,4}$\", i) or i == \"\":\n",
    "            if not df.filter(col(i).isNotNull()).count() >= 1:\n",
    "                df = df.drop(i)\n",
    "                dummy.append(h)\n",
    "                continue\n",
    "            df = df.withColumnRenamed(i, \"unnamed_\" + str(h))\n",
    "            h_none.append(h)\n",
    "    return df, h_none, dummy\n",
    "\n",
    "def logs(part, message):\n",
    "    now = datetime.datetime.now()\n",
    "    timestamp = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return [timestamp, part, message]\n",
    "\n",
    "\n",
    "def check_key(dic, key):\n",
    "    return True if key in dic.keys() else False\n",
    "\n",
    "\n",
    "def implementation_header_cleansing(header_cleansing, df):\n",
    "    row = []\n",
    "    if check_key(header_cleansing, \"Have Uppercase\"):\n",
    "        df = header_small(header_cleansing[\"Have Uppercase\"], df)\n",
    "        row.append(logs(\"Header\", \"UpperCase in {} headers are changed to LowerCase \".format([df.columns[i] for i in header_cleansing[\"Have Uppercase\"]])))\n",
    "\n",
    "    if check_key(header_cleansing, \"Have Space\"):\n",
    "        df = remove_spaces_add_underscore(header_cleansing[\"Have Space\"], df)\n",
    "        row.append(logs(\"Header\",\"Leading and trailing spaces in {} headers are removed and in between spaces are replaced with underscore\".format([df.columns[i] for i in header_cleansing[\"Have Space\"]])))\n",
    "\n",
    "    if check_key(header_cleansing, \"Have Special Characters\"):\n",
    "        df = remove_special_characters(header_cleansing[\"Have Special Characters\"], df)\n",
    "        row.append(logs(\"Header\", \"Special Characters in {} headers are removed  \".format([df.columns[i] for i in header_cleansing[\"Have Special Characters\"]])))\n",
    "\n",
    "    return df, row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed1dca67-eabf-46ec-8bf2-158d351d47a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "def check_null_values(df):\n",
    "\n",
    "    l = []\n",
    "    for i in df.columns:\n",
    "        if df.filter(col(i) == \"\").count() > 0:\n",
    "            l.append(i)\n",
    "\n",
    "    return l\n",
    "\n",
    "\n",
    "def date_col_string(df,count1=None):\n",
    "    date_columns = {}\n",
    "    if count1 is None:\n",
    "        count1=df.count()\n",
    "    if count1<10000:\n",
    "        df1=df\n",
    "    else:\n",
    "        df1=df.limit(10000)\n",
    "    for col_name, data_type in df.dtypes:\n",
    "        if data_type == 'date' or isinstance(data_type, DateType):\n",
    "            if \"datetype\" not in date_columns.keys():\n",
    "                date_columns[\"datetype\"]=[]\n",
    "            date_columns[\"datetype\"].append(col_name)\n",
    "            continue\n",
    "        else:\n",
    "            df1 = df1.withColumn(\"date check1\", regexp_replace(col(col_name), r\"[^0-9\\/\\-\\a-z\\.]\", \"\"))\n",
    "            df1 = df1.withColumn(\"date check2\", col(\"date check1\").rlike(\n",
    "                r\"^\\d{4}[/-]\\d{1,2}[/-]\\d{1,2}$|^\\d{1,2}[/-]\\d{1,2}[/-]\\d{4}$|^\\d{1,2}[/-]\\d{4}[/-]\\d{1,2}$\"))\n",
    "            if df1.filter(col(\"date check2\") == True).count() > 0:\n",
    "                if \"string\" not in date_columns.keys():\n",
    "                    date_columns[\"string\"]=[]\n",
    "                date_columns[\"string\"].append(col_name)\n",
    "                df = df.withColumn(col_name, regexp_replace(col(col_name), r\"[^0-9\\/\\-]\", \"\"))\n",
    "    df1 = df1.drop(\"date check1\", \"date check2\")\n",
    "    return df, date_columns\n",
    "\n",
    "\n",
    "def check_timestamp(df,count1):\n",
    "    timestamp_columns = {}\n",
    "    if count1 is None:\n",
    "        count1=df.count()\n",
    "    if count1<10000:\n",
    "        df1=df\n",
    "    else:\n",
    "        df1=df.limit(10000)\n",
    "    for col_name, data_type in df.dtypes:\n",
    "\n",
    "        if data_type == 'timestamp' or isinstance(data_type, TimestampType):\n",
    "            if \"timestamp\" not in timestamp_columns.keys():\n",
    "                timestamp_columns['timestamp']=[]\n",
    "            timestamp_columns['timestamp'].append(col_name)\n",
    "            continue\n",
    "        df1 = df1.withColumn(\"datetime check1\", regexp_replace(col(col_name), r\"[^0-9+TZ:./ \\-\\a-z]\", \"\"))\n",
    "        df1 = df1.withColumn(\"datetime check2\", col(\"datetime check1\").rlike(\n",
    "            r\"\\d{4}[/-]\\d{2}[-/]\\d{2}[T ]\\d{2}:\\d{2}:\\d{2}(\\.\\d+)?(Z|[+-]\\d{2}[:]\\d{2}|)\"))\n",
    "        if df1.filter(col(\"datetime check2\") == True).count() > 0:\n",
    "            if \"string\" not in timestamp_columns.keys():\n",
    "                timestamp_columns[\"string\"]=[]\n",
    "            timestamp_columns[\"string\"].append(col_name)\n",
    "            df = df.withColumn(col_name, regexp_replace(col(col_name), r\"[^0-9+TZ:./ -]\", \"\"))\n",
    "        df1 = df1.drop(\"datetime check2\", \"datetime check1\")\n",
    "    return df, timestamp_columns\n",
    "\n",
    "\n",
    "def check_date(df, columnlist):\n",
    "    if len(columnlist) >0:\n",
    "        for i in columnlist.keys():\n",
    "            for column in columnlist[i]:\n",
    "                # iso_pattern = r'^\\d{4}-\\d{2}-\\d{2}$'\n",
    "                is_iso = to_date(col(column).cast('string'), \"yyyy-MM-dd\").isNotNull()\n",
    "                # Add the new column to the DataFrame\n",
    "                df = df.withColumn(column + '_is_iso', when(col(column)!=None, to_date(col(column).cast('string'), \"yyyy-MM-dd\").isNotNull()).otherwise(True))\n",
    "                if df.filter(col(column + '_is_iso') == False).count() > 0 or i == \"string\":\n",
    "                    df = df.drop(column + '_is_iso')\n",
    "                else:\n",
    "                    columnlist[i].remove(column)\n",
    "                    df = df.drop(column + '_is_iso')\n",
    "        return columnlist\n",
    "    else:\n",
    "        return columnlist\n",
    "\n",
    "\n",
    "def check_time(df, columnlist):\n",
    "    if len(columnlist) >0:\n",
    "        for i in columnlist.keys():\n",
    "            for column in columnlist[i]:\n",
    "                iso_pattern = r'\\d{4}-\\d{2}-\\d{2}[T ]\\d{2}:\\d{2}:\\d{2}(\\.\\d+)?(Z|[+-]\\d{2}:\\d{2}|)'\n",
    "                is_iso = regexp_extract(col(column).cast('string'), iso_pattern, 0).isNotNull()\n",
    "                # Add the new column to the DataFrame\n",
    "                df = df.withColumn(column + '_is_iso', when(col(column)!= None, regexp_extract(col(column).cast('string'), iso_pattern, 0).isNotNull()).otherwise(True))\n",
    "                if df.filter(col(column + '_is_iso') == False).count() > 0 or i == \"string\":\n",
    "                    df = df.drop(column + '_is_iso')\n",
    "                else:\n",
    "                    columnlist[i].remove(column)\n",
    "                    df = df.drop(column + '_is_iso')\n",
    "        return columnlist\n",
    "    else:\n",
    "        return columnlist\n",
    "\n",
    "\n",
    "def check_duplicates(df, primary_key=None, composite_keys=None,count1=None):\n",
    "    if count1 is None:\n",
    "        count1=df.count()\n",
    "    if primary_key is not None:\n",
    "        df=df.dropDuplicates([primary_key])\n",
    "        count2=df.count()\n",
    "        if count1 >count2 :\n",
    "            return df,logs('Data', '{} Duplicates are removed'.format(count1-count2))\n",
    "        else:\n",
    "            return df,logs('Data', '0 Duplicates are removed')\n",
    "    elif composite_keys is not None:\n",
    "        df=df.dropDuplicates(composite_keys)\n",
    "        count2=df.count()\n",
    "        if count1 >count2:\n",
    "            return df,logs('Data', '{} Duplicates are removed'.format(count1-count2))\n",
    "        else:\n",
    "            return df,logs('Data', '0 Duplicates are removed')\n",
    "    else:\n",
    "        df=df.distinct()\n",
    "        count2=df.count()\n",
    "        if count1 >count2 :\n",
    "            return df,logs('Data', '{} Duplicates are removed'.format(count1-count2))\n",
    "        else:\n",
    "            return df,logs('Data', '0 Duplicates are removed')\n",
    "\n",
    "\n",
    "def check_leading_trailing_spaces(df):\n",
    "    l = []\n",
    "    for i in df.columns:\n",
    "        df = df.withColumn(\"hasspace\", regexp_extract(col(i), r'^\\s+|\\s+$', 0).rlike(r'^\\s+|\\s+$'))\n",
    "        if df.filter(col(\"hasspace\") == True).count() > 0:\n",
    "            l.append(i)\n",
    "    df = df.drop(\"hasspace\")\n",
    "    return l\n",
    "\n",
    "def place(df,list2,columns=None,reg_ex=None):\n",
    "    vs={}\n",
    "    if columns is None:\n",
    "        columns=df.columns\n",
    "    if reg_ex is None:\n",
    "        reg_ex='[a-zA-Z0-9\\@\\;\\:\\,\\/\\\\\\.\\_\\ \\-\\.]'\n",
    "    for i in list2.keys():\n",
    "        if len(i)>1:\n",
    "            m_reg='\\\\'.join(i.split(\",\"))\n",
    "            lt_reg='[\\\\'+'\\\\'.join(i.split(\",\"))+']'\n",
    "\n",
    "        else:\n",
    "           lt_reg= m_reg='\\{}'.format(i)\n",
    "        if type(list2[i][0]) is str:\n",
    "            if i not in vs.keys():\n",
    "                    vs[i] = []\n",
    "            reg_ex1 = '^({})?[a-zA-Z0-9]+'.format(lt_reg)+reg_ex+'*[a-zA-Z0-9]+({})?$'.format(lt_reg)\n",
    "            for j in list2[i]:\n",
    "                if df.where(col(j).rlike(reg_ex1)==False).count() > 0:\n",
    "                    vs[i].append(j)\n",
    "                columns.remove(j)\n",
    "            continue\n",
    "        if list2[i][0]==1:\n",
    "            if i not in vs.keys():\n",
    "                vs[i] = [1]\n",
    "            reg_ex1 = '^({})?[a-zA-Z0-9]+'.format(lt_reg)+reg_ex+'*[a-zA-Z0-9]+$'\n",
    "            for j in list2[i][1:]:\n",
    "                if df.where(col(j).rlike(reg_ex1)==False).count() > 0:\n",
    "                    vs[i].append(j)\n",
    "                columns.remove(j)\n",
    "            continue\n",
    "        if list2[i][0]==2:\n",
    "            if i not in vs.keys():\n",
    "                    vs[i] = [2]\n",
    "            reg_ex1 ='^[a-zA-Z0-9]+'+reg_ex[:-1] + \"{}\".format(str(m_reg)) + reg_ex[-1:]+'*[a-zA-Z0-9]+$'\n",
    "            for j in list2[i][1:]:\n",
    "                if df.where(col(j).rlike(reg_ex1)==False).count() > 0:\n",
    "                    vs[i].append(j)\n",
    "                columns.remove(j)\n",
    "            continue\n",
    "        if list2[i][0]==3:\n",
    "            if i not in vs.keys():\n",
    "                    vs[i] = [3]\n",
    "            reg_ex1 = '^[a-zA-Z0-9]+'+reg_ex+'*[a-zA-Z0-9]+({})?$'.format(lt_reg)\n",
    "            for j in list2[i][1:]:\n",
    "                print(reg_ex1)\n",
    "                if df.where(col(j).rlike(reg_ex1)==False).count() > 0:\n",
    "                    vs[i].append(j)\n",
    "                columns.remove(j)\n",
    "            continue\n",
    "        if list2[i][0]==4:\n",
    "            if i not in vs.keys():\n",
    "                    vs[i] = [4]\n",
    "            reg_ex1 = '^({})?[a-zA-Z0-9]+'.format(lt_reg)+reg_ex[:-1] + \"{}\".format(str(m_reg)) + reg_ex[-1:]+'*[a-zA-Z0-9]+$'\n",
    "            for j in list2[i][1:]:\n",
    "                if df.where(col(j).rlike(reg_ex1)==False).count() > 0:\n",
    "                    vs[i].append(j)\n",
    "                columns.remove(j)\n",
    "            continue\n",
    "        if list2[i][0]==5:\n",
    "            if i not in vs.keys():\n",
    "                    vs[i] = [5]\n",
    "            reg_ex1 = '^[a-zA-Z0-9]+'+reg_ex[:-1] + \"{}\".format(str(m_reg)) + reg_ex[-1:]+'*[a-zA-Z0-9]+({})?$'.format(lt_reg)\n",
    "            for j in list2[i][1:]:\n",
    "                if df.where(col(j).rlike(reg_ex1)==False).count() > 0:\n",
    "                    vs[i].append(j)\n",
    "                columns.remove(j)\n",
    "            continue\n",
    "        if list2[i][0]==6:\n",
    "            if i not in vs.keys():\n",
    "                    vs[i] = [6]\n",
    "            reg_ex1 = '^({})?[a-zA-Z0-9]+'.format(lt_reg)+reg_ex[:-1] + \"\\{}\".format(str(m_reg)) + reg_ex[-1:]+'*[a-zA-Z0-9]+({})?$'.format(lt_reg)\n",
    "            for j in list2[i][1:]:\n",
    "                if df.where(col(j).rlike(reg_ex1)==False).count() > 0:\n",
    "                    vs[i].append(j)\n",
    "                columns.remove(j)\n",
    "    return columns,vs\n",
    "        \n",
    "def check_special_char(df, valid_across_allcol=None,date_columns=None, timestamp_columns=None, list2=None):\n",
    "    columns = df.columns\n",
    "    l = []\n",
    "    vs={}\n",
    "    reg_ex = r\"[a-zA-Z0-9@\\;\\:\\,\\/\\\\\\.\\_\\ \\-]\"\n",
    "    if valid_across_allcol != None:\n",
    "        for i in valid_across_allcol:\n",
    "            reg_ex = reg_ex[:-1] + \"\\{}\".format(str(i)) + reg_ex[-1:]\n",
    "    if date_columns is not None and \"string\" in date_columns.keys():\n",
    "        for i in date_columns[\"string\"]:\n",
    "            columns.remove(i)\n",
    "    if timestamp_columns is not None and \"string\" in timestamp_columns.keys():\n",
    "        for i in timestamp_columns[\"string\"]:\n",
    "            columns.remove(i)\n",
    "    if list2 is not None:\n",
    "        columns,vs=place(df,list2,columns,reg_ex)  \n",
    "    reg_ex1='^[a-zA-Z0-9]+'+reg_ex+'*[a-zA-Z0-9]'\n",
    "    for i in columns:\n",
    "        if df.where(col(i).rlike(reg_ex1)==False).count() > 0:\n",
    "            l.append(i)\n",
    "    return l,vs, reg_ex\n",
    "\n",
    "def not_valid_primarykey_compositekey(df, primary_key=None, composite_keys=None):\n",
    "    output = {}\n",
    "    if composite_keys != None:\n",
    "        df = df.withColumn(\"composite_key\",\n",
    "                           concat_ws(\"\", *[coalesce(col(c).cast(StringType()), lit(\"\")) for c in composite_keys]))\n",
    "        if df.filter(col(\"composite_key\") == \"\").count() > 0:\n",
    "            output[\"Valid Composite key\"] = \"not valid as they have atleast 1 null value in combination\"\n",
    "\n",
    "        else:\n",
    "            output[\"Valid Composite key\"] = \"valid as they don't have any null value in combination\"\n",
    "    else:\n",
    "        output[\"Valid Composite key\"] = \"not passed/given\"\n",
    "    if primary_key != None:\n",
    "        if df.filter(col(primary_key).isNull()).count() > 0:\n",
    "            output[\"Valid Primary key\"] = \"not valid as it does have atleast 1 null value\"\n",
    "        else:\n",
    "            output[\"Valid Primary key\"] = \"valid as it doesn't have any null value\"\n",
    "    else:\n",
    "        output[\"Valid Primary key\"] = \"not passed/given\"\n",
    "    return output\n",
    "def check_datatype(df):\n",
    "    Integer=[]\n",
    "    Float=[]\n",
    "    for i, data_type in df.dtypes:\n",
    "       if data_type == 'string' or isinstance(data_type, StringType):\n",
    "           if df.where(col(i).rlike(\"^[0-9]*\\.[0-9]*$\")==False).count()==0:\n",
    "               Float.append(i)\n",
    "               continue\n",
    "           if df.where(col(i).rlike(\"^[0-9]*$\")==False).count()==0:\n",
    "               Integer.append(i)\n",
    "               continue\n",
    "    return Integer,Float\n",
    "\n",
    "    \n",
    "def azure_connection(path,azure_blob):\n",
    "    conf = SparkConf()\n",
    "    conf.set(\"spark.jars.packages\",\n",
    "             \"org.apache.hadoop:hadoop-azure:3.3.5,com.microsoft.azure:azure-storage:8.6.6,com.azure:azure-storage-blob:12.24.0\")\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[1]\") \\\n",
    "        .appName(\"Read CSV\") \\\n",
    "        .config(conf=conf) \\\n",
    "        .config(\"fs.azure.account.key.{}.blob.core.windows.net\".format(azure_blob[\"storage_account_name\"]), azure_blob[\"storage_account_key\"]) \\\n",
    "        .getOrCreate()\n",
    "    df = spark.read.format('csv').option('header', True).load(\"wasbs://{}@{}.blob.core.windows.net/{}\".format(azure_blob[\"container_name\"],azure_blob[\"storage_account_name\"],path))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbb90bbc-ce53-4b20-b932-ede4ebf2b1d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import datetime\n",
    "\n",
    "def change_datatype(df,Integer,Float):\n",
    "    for i in Integer:\n",
    "        df=df.withColumn(i,col(i).cast(\"integer\"))\n",
    "    for i in Float:\n",
    "        df=df.withColumn(i,col(i).cast(\"float\"))\n",
    "    return df\n",
    "\n",
    "def trim_leading_trailing_spaces(df, column_names):\n",
    "    for i in column_names:\n",
    "        df = df.withColumn(i, trim(df[i]))\n",
    "    return df\n",
    "\n",
    "\n",
    "def missing_values(df, column_names):\n",
    "    for i in column_names:\n",
    "        df = df.withColumn(i, when(col(i).isNull(), None).when(col(i) == '', None).otherwise(col(i)))\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_duplicates(df, primary_key=None, composite_keys=None):\n",
    "    if primary_key != None:\n",
    "        return df.dropDuplicates([primary_key])\n",
    "    if composite_keys != None:\n",
    "        return df.dropDuplicates(composite_keys)\n",
    "    else:\n",
    "        return df.distinct()\n",
    "\n",
    "\n",
    "\n",
    "def date_iso(df, date_columns):\n",
    "    if \"string\" in date_columns.keys() and len(date_columns[\"string\"]) > 0:\n",
    "        for i in date_columns[\"string\"]:\n",
    "            df = df.withColumn(i, when(to_date(col(i), 'd/M/yyyy').isNotNull(), to_date(col(i), 'd/M/yyyy'))\n",
    "                               .when(to_date(col(i), 'M/d/yyyy').isNotNull(), to_date(col(i), 'M/d/yyyy'))\n",
    "                               .when(to_date(col(i), 'MM/dd/yyyy').isNotNull(), to_date(col(i), 'MM/dd/yyyy'))\n",
    "                               .when(to_date(col(i), 'M-d-yyyy').isNotNull(), to_date(col(i), 'M-d-yyyy'))\n",
    "                               .when(to_date(col(i), 'd-M-yyyy').isNotNull(), to_date(col(i), 'd-M-yyyy'))\n",
    "                               .when(to_date(col(i), 'dd-MM-yyyy').isNotNull(), to_date(col(i), 'dd-MM-yyyy'))\n",
    "                               .when(to_date(col(i), 'dd/MM/yyyy').isNotNull(), to_date(col(i), 'dd/MM/yyyy'))\n",
    "                               .when(to_date(col(i), 'yyyy/MM/dd').isNotNull(), to_date(col(i), 'yyyy/MM/dd'))\n",
    "                               .when(to_date(col(i), 'yyyy/dd/MM').isNotNull(), to_date(col(i), 'yyyy/dd/MM'))\n",
    "                               .when(to_date(col(i), 'yyyy-MM-dd').isNotNull(), to_date(col(i), 'yyyy-MM-dd'))\n",
    "                               .when(to_date(col(i), 'yyyy-dd-MM').isNotNull(), to_date(col(i), 'yyyy-dd-MM'))\n",
    "                               .otherwise(col(i)))\n",
    "            df = df.withColumn(i, to_date(col(i)))\n",
    "\n",
    "    if \"datetype\" in date_columns.keys() and len(date_columns[\"datetype\"]):\n",
    "        for i in date_columns[\"datetype\"]:\n",
    "            df = df.withColumn(i, date_format(col(i), \"yyyy-mm-dd\"))\n",
    "            df = df.withColumn(i, to_date(col(i)))\n",
    "    return df\n",
    "\n",
    "def place_remove(df,list2,reg_ex=None):\n",
    "    if reg_ex is None:\n",
    "        reg_ex='[^a-zA-Z0-9\\@\\;\\:\\,\\/\\\\\\.\\_\\ \\-\\.]'\n",
    "    for i in list2.keys():\n",
    "        if len(i)>1:\n",
    "            m_reg='\\\\'.join(i.split(\",\"))\n",
    "            lt_reg='[\\\\'+'\\\\'.join(i.split(\",\"))+']'\n",
    "        else:\n",
    "           lt_reg= m_reg='\\{}'.format(i)\n",
    "        if type(list2[i][0]) is str:\n",
    "            for j in list2[i]:\n",
    "                df = df.withColumn(j, when(col(j).isNull(),None).otherwise(concat_ws(\"\",regexp_extract(regexp_extract(col(j), r'^[^a-zA-Z0-9]*', 0), lt_reg,0),regexp_replace(regexp_replace(col(j), '^[^a-zA-Z0-9]*|[^a-zA-Z0-9]*$', ''), reg_ex,\"\"),regexp_extract(regexp_extract(col(j), r'[^a-zA-Z0-9]*$', 0), lt_reg,0))))\n",
    "                df.show()\n",
    "            continue\n",
    "        if list2[i][0]==1:\n",
    "            \n",
    "            for j in list2[i][1:]:\n",
    "                df = df.withColumn(j, when(col(j).isNull(),None).otherwise(concat_ws(\"\",regexp_extract(regexp_extract(col(j), r'^[^a-zA-Z0-9]*', 0), lt_reg,0),regexp_replace(regexp_replace(col(j), '^[^a-zA-Z0-9]*|[^a-zA-Z0-9]*$', ''), reg_ex,\"\"))))\n",
    "            continue\n",
    "        if list2[i][0]==2:\n",
    "            reg_ex1 = reg_ex[:-1] + m_reg + reg_ex[-1:]\n",
    "            for j in list2[i][1:]:\n",
    "                df = df.withColumn(j, when(col(j).isNull(),None).otherwise(regexp_replace(regexp_replace(col(j), '^[^a-zA-Z0-9]*|[^a-zA-Z0-9]*$', ''), reg_ex1, \"\")))  \n",
    "            continue\n",
    "        if list2[i][0]==3:\n",
    "            for j in list2[i][1:]:\n",
    "                df = df.withColumn(j,when(col(j).isNull(),None).otherwise(concat_ws(\"\",regexp_replace(regexp_replace(col(j), '^[^a-zA-Z0-9]*|[^a-zA-Z0-9]*$', ''), reg_ex,\"\"),regexp_extract(regexp_extract(col(j), r'[^a-zA-Z0-9]*$', 0), lt_reg,0))))\n",
    "            continue\n",
    "        if list2[i][0]==4:\n",
    "            reg_ex1 = reg_ex[:-1] + m_reg + reg_ex[-1:]\n",
    "            for j in list2[i][1:]:\n",
    "                df = df.withColumn(j,when(col(j).isNull(),None).otherwise(concat_ws(\"\",\n",
    "                                                regexp_extract(regexp_extract(col(j), r'^[^a-zA-Z0-9]*', 0), lt_reg,\n",
    "                                                               0),\n",
    "                                                regexp_replace(regexp_replace(col(j), '^[^a-zA-Z0-9]*|[^a-zA-Z0-9]*$', ''), reg_ex1,\n",
    "                                                               \"\"))))\n",
    "            continue\n",
    "        if list2[i][0]==5:\n",
    "            reg_ex1 = reg_ex[:-1] + m_reg + reg_ex[-1:]\n",
    "            for j in list2[i][1:]:\n",
    "                df = df.withColumn(j,when(col(j).isNull(),None).otherwise(concat_ws(\"\",regexp_replace(regexp_replace(col(j), '^[^a-zA-Z0-9]*|[^a-zA-Z0-9]*$', ''), reg_ex1,\"\"),regexp_extract(regexp_extract(col(j), r'[^a-zA-Z0-9]*$', 0), lt_reg,0))))\n",
    "            continue\n",
    "        if list2[i][0]==6:\n",
    "            reg_ex1 = reg_ex[:-1] + m_reg + reg_ex[-1:]\n",
    "            for j in list2[i][1:]:\n",
    "                df = df.withColumn(j,when(col(j).isNull(),None).otherwise(concat_ws(\"\",\n",
    "                                                regexp_extract(regexp_extract(col(j), r'^[^a-zA-Z0-9]*', 0), lt_reg,\n",
    "                                                               0),\n",
    "                                                regexp_replace(regexp_replace(col(j), '^[^a-zA-Z0-9]*|[^a-zA-Z0-9]*$', ''), reg_ex1,\n",
    "                                                               \"\"),\n",
    "                                                regexp_extract(regexp_extract(col(j), r'[^a-zA-Z0-9]*$', 0), lt_reg,\n",
    "                                                               0))))\n",
    "    return df\n",
    "        \n",
    "\n",
    "\n",
    "def remove_l_t_valid_special_characters(list1, df, list2=None, reg_ex=None,valid_across_allcol=None):\n",
    "    \"\"\"list 2 is from config \"\"\"  # {\"@\":[col_name,col_name]}\n",
    "    \"\"\" list 1 will have column names that contain leading and trailing special chars leaving the list2 columns   \"\"\"\n",
    "    if reg_ex is None:\n",
    "        reg_ex = r\"[^a-zA-Z0-9@\\;\\:\\,\\/\\\\\\.\\_\\ \\-]\"\n",
    "        if valid_across_allcol != None:\n",
    "            for i in valid_across_allcol:\n",
    "                reg_ex = reg_ex[:-1] + \"\\{}\".format(str(i)) + reg_ex[-1:]\n",
    "\n",
    "    if list2 is not None:\n",
    "\n",
    "        df=place_remove(df,list2,reg_ex)\n",
    "\n",
    "        if len(list1) > 0:\n",
    "            for i in list1:\n",
    "                df = df.withColumn(i, regexp_replace(col(i), r'^[^a-zA-Z0-9]*|[^a-zA-Z0-9@\\;\\:\\,\\/\\\\\\.\\_\\ \\-]|[^a-zA-Z0-9]+$', ''))\n",
    "        return df\n",
    "    else:\n",
    "        for i in list1:\n",
    "            df = df.withColumn(i, regexp_replace(col(i), r'^[^a-zA-Z0-9]*|{}|[^a-zA-Z0-9]+$'.format(reg_ex), ''))\n",
    "        return df\n",
    "\n",
    "\n",
    "def to_iso(df, timestamp_columns, inputtz=None, outputtz=None):\n",
    "    if inputtz is None:\n",
    "        inputtz = \"GMT\"\n",
    "    if outputtz is None:\n",
    "        outputtz = \"UTC\"\n",
    "    output_format = 'yyyy-MM-dd\\'T\\'HH:mm:ss.SSS\\'Z\\''\n",
    "    collist = []\n",
    "    for i in timestamp_columns.values():\n",
    "        collist += i\n",
    "    for colname in collist:\n",
    "        # Convert the input timestamp to UTC\n",
    "        utc_time = to_utc_timestamp(col(colname), inputtz)\n",
    "        # Convert the UTC timestamp to the output format\n",
    "        iso_time = from_utc_timestamp(utc_time, outputtz).cast('string')\n",
    "        # Remove any trailing zeros from the seconds\n",
    "        iso_time = regexp_replace(iso_time, '\\.0*Z$', 'Z')\n",
    "        # Add the new column to the DataFrame\n",
    "        df = df.withColumn(colname, to_timestamp(date_format(iso_time, output_format)))\n",
    "        df = df.withColumn(colname, to_timestamp(colname))\n",
    "    return df\n",
    "\n",
    "\n",
    "def check_key(dic, key):\n",
    "    return True if key in dic.keys() else False\n",
    "\n",
    "\n",
    "def logs(part, message):\n",
    "    now = datetime.datetime.now()\n",
    "    timestamp = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return [timestamp, part, message]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74ee6a9e-0ec8-4d60-b3b0-caa58b01682f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def curate_data(path, logging_path=None, output_file_path=None, primary_key=None, composite_keys=None,\n",
    "                valid_across_allcol=None, valid_spl_chars=None, inputtz=None, outputtz=None, aws_s3=None,azure_blob=None):\n",
    "    try:\n",
    "        if path is None:\n",
    "            raise ValueError(\"No path given.\")\n",
    "        if urlparse(path).scheme in ['https', 'http']:\n",
    "            spark = SparkSession.builder.appName(\"Read CSV\").getOrCreate()\n",
    "            spark.sparkContext.addFile(path)\n",
    "            df = spark.read.format(\"csv\").option(\"header\", True).load(\"file://\" + SparkFiles.get(\n",
    "                re.search(r\"/([^/]+\\.csv)$\", path).group(1) if re.search(r\"/([^/]+\\.csv)$\", path) else None))\n",
    "        elif urlparse(path).scheme == 's3':\n",
    "            conf = SparkConf()\n",
    "            conf.set(\"spark.jars.packages\",\n",
    "                     \"org.apache.spark:spark-hadoop-cloud_2.12:2.12.14,org.apache.hadoop:hadoop-aws:3.0.0,org.apache.hadoop:hadoop-client:3.0.0,org.apache.hadoop:hadoop-common:3.0.0,com.amazonaws:aws-java-sdk-core:2.20.97,com.amazonaws:aws-java-sdk-kms:1.12.490,com.amazonaws:aws-java-sdk-s3:1.12.467\")\n",
    "            spark = SparkSession.builder \\\n",
    "                .master(\"local[1]\") \\\n",
    "                .appName(\"Read CSV\") \\\n",
    "                .config(conf=conf) \\\n",
    "                .config(\"fs.s3a.access.key\", aws_s3[\"access_key\"]) \\\n",
    "                .config(\"fs.s3a.secret.key\", aws_s3[\"secret_key\"]) \\\n",
    "                .config(\"fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
    "                .getOrCreate()\n",
    "            path = \"s3a\" + path[2:]\n",
    "            df = spark.read.format(\"csv\").option(\"header\", True).load(path)\n",
    "        elif azure_blob is not None:\n",
    "            df=azure_connection(path,azure_blob)\n",
    "        else:\n",
    "            spark = SparkSession.builder.appName(\"Read CSV\").getOrCreate()\n",
    "            df = spark.read.format(\"csv\").option(\"header\", True).load(path)\n",
    "        if df.rdd.isEmpty():\n",
    "            raise ValueError(\"File is empty\")\n",
    "        count1 = df.count()\n",
    "        if df.columns and count1 == 0:\n",
    "            raise ValueError(\"DataFrame has only column names and no data.\")\n",
    "    except FileNotFoundError:\n",
    "        raise ValueError(\"File/File path Not Found\")\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        exit()\n",
    "    try:\n",
    "        if all(header == \"\" for header in df.columns):\n",
    "            raise Exception(\"All headers are null.\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        exit()\n",
    "    for j in valid_spl_chars.keys():\n",
    "        if type(valid_spl_chars[j][0]) is str:\n",
    "            valid_spl_chars[j]=[\"%Cache\\\\Value&#\"]+valid_spl_chars[j]\n",
    "    for i, column_name in enumerate(df.columns):\n",
    "        if primary_key is not None:\n",
    "            if primary_key == column_name:\n",
    "                primary_key = i\n",
    "        if composite_keys is not None and column_name in composite_keys:\n",
    "            composite_keys[composite_keys.index(column_name)] = i\n",
    "        if valid_spl_chars is not None:\n",
    "                if column_name in valid_spl_chars[j]:\n",
    "                    valid_spl_chars[j][valid_spl_chars[j].index(column_name)] = i\n",
    "    \n",
    "    header_cleansing_columns = implement_header_validation(df)\n",
    "\n",
    "    if len(header_cleansing_columns) > 0:\n",
    "        df, row = implementation_header_cleansing(header_cleansing_columns, df)\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"There is no Header cleansing Required\")\n",
    "        now = datetime.datetime.now()\n",
    "        ts = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        row = [[ts, \"Header\", \"There is no Header cleansing Required\"]]\n",
    "    df, h_none, dummy = change_header(df, count1)\n",
    "    if h_none is not None:\n",
    "        now = datetime.datetime.now()\n",
    "        ts = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        row.append([ts, \"Header\",\n",
    "                    \"Null header names of indices {} are changed to appropriate name(unnamed_index)\".format(h_none)])\n",
    "    if dummy is not None:\n",
    "        now = datetime.datetime.now()\n",
    "        ts = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        row.append([ts, \"Header\",\n",
    "                    \"Headers that are null & data of that column is entirely null are in  indices {} are deleted\".format(\n",
    "                        dummy)])\n",
    "    for i, column in enumerate(df.columns):\n",
    "        if primary_key is not None and primary_key == i:\n",
    "            primary_key = column\n",
    "        if composite_keys is not None and i in composite_keys:\n",
    "            composite_keys[composite_keys.index(i)] = column\n",
    "        if valid_spl_chars is not None:\n",
    "                if i in valid_spl_chars[j][1:]:\n",
    "                    valid_spl_chars[j][valid_spl_chars[j].index(i)] = column\n",
    "    for j in valid_spl_chars.keys():\n",
    "\n",
    "        if valid_spl_chars[j][0]==\"%Cache\\\\Value&#\":\n",
    "            valid_spl_chars[j]=valid_spl_chars[j][1:]\n",
    "    rows = []\n",
    "\n",
    "    df, date_columns = date_col_string(df, count1)\n",
    "    df, timestamp_columns = check_timestamp(df, count1)\n",
    "\n",
    "    space_col = check_leading_trailing_spaces(df)\n",
    "    if len(space_col) > 0:\n",
    "        df = trim_leading_trailing_spaces(df, space_col)\n",
    "        rows.append(logs(\"Data\", \"Leading and trailing spaces in {} columns are removed\".format(space_col)))\n",
    "\n",
    "    spl_char_col, v_s, reg_ex = check_special_char(df, valid_across_allcol, date_columns, timestamp_columns,valid_spl_chars)\n",
    "    cache=list(v_s.keys())\n",
    "    for i in cache:\n",
    "        if len(v_s[i])<1:\n",
    "            v_s.pop(i)\n",
    "        elif len(v_s[i])<2 and type(v_s[i][0]) is int :\n",
    "            v_s.pop(i)\n",
    "        else:\n",
    "            continue\n",
    "    if len(spl_char_col) > 0 or len(v_s) > 0:\n",
    "        df = remove_l_t_valid_special_characters(spl_char_col, df, v_s, None, valid_across_allcol)\n",
    "        rows.append(logs(\"Data\", \"In valid Special_characters in {} columns are removed\".format(spl_char_col)))\n",
    "        rows.append(logs(\"Data\",\n",
    "                         \"Leading and trailing invalid special characters in {} columns are removed and the key(special Character) of dictionary is kept in the leading and trailing according to the {} \".format(\n",
    "                             v_s, valid_spl_chars)))\n",
    "\n",
    "    null_col = check_null_values(df)\n",
    "    if len(null_col) > 0:\n",
    "        df = missing_values(df, null_col)\n",
    "        rows.append(logs(\"Data\", \"Missing values in {} columns are replaced with null\".format(null_col)))\n",
    "\n",
    "    if len(date_columns) > 0:\n",
    "        change_date = check_date(df, date_columns)\n",
    "        if len(change_date[\"string\"]) > 0 or len(change_date[\"datetype\"]) > 0:\n",
    "            df = date_iso(df, change_date)\n",
    "            rows.append(logs(\"Data\", \"Date format in {} columns are changed to iso format\".format(change_date)))\n",
    "\n",
    "    if len(timestamp_columns) > 0:\n",
    "        change_datetime = check_time(df, timestamp_columns)\n",
    "        if len(change_datetime[\"string\"]) > 0 or len(change_datetime[\"timestamp\"]) > 0:\n",
    "            df = to_iso(df, change_datetime, inputtz, outputtz)\n",
    "            rows.append(\n",
    "                logs(\"Data\", \"Date Time format in {} columns are changed to iso format\".format(change_datetime)))\n",
    "    Integer,Float=check_datatype(df)\n",
    "    if len(Integer)>0 or len(Float)>0:\n",
    "        df=change_datatype(df,Integer,Float)\n",
    "        if len(Integer)>0:\n",
    "            rows.append(logs(\"Data\", \"{} columns are changed from string to integer format\".format(Integer)))\n",
    "        if len(Float)>0:\n",
    "            rows.append(logs(\"Data\", \"{} columns are changed from string to float format\".format(Float)))\n",
    "    output = not_valid_primarykey_compositekey(df, primary_key, composite_keys)\n",
    "    rows.append(logs(\"Data\", \"Primary Key is {}\".format(output[\"Valid Primary key\"])))\n",
    "    rows.append(logs(\"Data\", \"Composite Key is {}\".format(output[\"Valid Composite key\"])))\n",
    "    df,x= check_duplicates(df, primary_key, composite_keys, count1)\n",
    "    rows.append(x)\n",
    "    if len(rows) == 0:\n",
    "        print(\"There is no Data cleansing Required\")\n",
    "        now = datetime.datetime.now()\n",
    "        ts = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        rows = [[ts, 'Data', 'There is no Data cleansing Required']]\n",
    "    rows = row + rows\n",
    "    now = datetime.datetime.now()\n",
    "    ts = now.strftime(\"%Y/%m/%d/%H/%M_%S\")\n",
    "    d = spark.createDataFrame(rows, [\"Timestamp\", \"Cleansing Part\", \"Message\"])\n",
    "\n",
    "    if logging_path is None:\n",
    "        if output_file_path is None:\n",
    "            return df, d\n",
    "        file_name = os.path.basename(path)\n",
    "        df.write.parquet(output_file_path + f\"output/{ts}\" + file_name[:-4])\n",
    "        return d\n",
    "    else:\n",
    "        file_name = os.path.basename(path)\n",
    "        output_file = f\"logfile/{ts}_{file_name[:-4]}\"\n",
    "        d.coalesce(1).write.option(\"header\", True).csv(logging_path + output_file)\n",
    "        if output_file_path is None:\n",
    "            return df\n",
    "        file_name = os.path.basename(path)\n",
    "        df.write.option(\"header\", \"true\").mode(\"overwrite\").parquet(output_file_path + f\"output/{ts}\" + file_name[:-4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2200622d-50c9-4cd7-82ee-e505440c9e1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df,d= curate_data(path=\"/FileStore/tables/export_100000-1.csv\", composite_keys=[\"first name\",\"laSt_name\"], valid_spl_chars={\"$\":[1,\"salary\"],\"#\":[\"color_hex\"],\"+/-\":[4,\"latitude\",\"longitude\"]})\n",
    "\n",
    "df, d = curate_data(\n",
    "    path=\"https://gist.githubusercontent.com/kevin336/acbb2271e66c10a5b73aacf82ca82784/raw/e38afe62e088394d61ed30884dd50a6826eee0a8/employees.csv\",\n",
    "    primary_key=\"EMPLOYEE_ID\",\n",
    "    composite_keys=[\"FIRST_NAME\", \"LAST_NAME\"],\n",
    "    valid_spl_chars={\n",
    "        \"$\": [1, \"SALARY\"],\n",
    "        # \".\": [\"COMMISSION_PCT\"],\n",
    "        # \"-\": [\"PHONE_NUMBER\"]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c604da40-f350-4fda-a67b-a63ee49e0dfe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------+--------+------------+---------+----------+------+--------------+----------+-------------+\n|employee_id|first_name |last_name |email   |phone_number|hire_date|job_id    |salary|commission_pct|manager_id|department_id|\n+-----------+-----------+----------+--------+------------+---------+----------+------+--------------+----------+-------------+\n|100        |Steven     |King      |SKING   |515.123.4567|17-JUN-03|AD_PRES   |24000 |null          |null      |90           |\n|101        |Neena      |Kochhar   |NKOCHHAR|515.123.4568|21-SEP-05|AD_VP     |17000 |null          |100       |90           |\n|102        |Lex        |De Haan   |LDEHAAN |515.123.4569|13-JAN-01|AD_VP     |17000 |null          |100       |90           |\n|103        |Alexander  |Hunold    |AHUNOLD |590.423.4567|03-JAN-06|IT_PROG   |9000  |null          |102       |60           |\n|104        |Bruce      |Ernst     |BERNST  |590.423.4568|21-MAY-07|IT_PROG   |6000  |null          |103       |60           |\n|105        |David      |Austin    |DAUSTIN |590.423.4569|25-JUN-05|IT_PROG   |4800  |null          |103       |60           |\n|106        |Valli      |Pataballa |VPATABAL|590.423.4560|05-FEB-06|IT_PROG   |4800  |null          |103       |60           |\n|107        |Diana      |Lorentz   |DLORENTZ|590.423.5567|07-FEB-07|IT_PROG   |4200  |null          |103       |60           |\n|108        |Nancy      |Greenberg |NGREENBE|515.124.4569|17-AUG-02|FI_MGR    |12008 |null          |101       |100          |\n|109        |Daniel     |Faviet    |DFAVIET |515.124.4169|16-AUG-02|FI_ACCOUNT|9000  |null          |108       |100          |\n|110        |John       |Chen      |JCHEN   |515.124.4269|28-SEP-05|FI_ACCOUNT|8200  |null          |108       |100          |\n|111        |Ismael     |Sciarra   |ISCIARRA|515.124.4369|30-SEP-05|FI_ACCOUNT|7700  |null          |108       |100          |\n|112        |Jose Manuel|Urman     |JMURMAN |515.124.4469|07-MAR-06|FI_ACCOUNT|7800  |null          |108       |100          |\n|113        |Luis       |Popp      |LPOPP   |515.124.4567|07-DEC-07|FI_ACCOUNT|6900  |null          |108       |100          |\n|114        |Den        |Raphaely  |DRAPHEAL|515.127.4561|07-DEC-02|PU_MAN    |11000 |null          |100       |30           |\n|115        |Alexander  |Khoo      |AKHOO   |515.127.4562|18-MAY-03|PU_CLERK  |3100  |null          |114       |30           |\n|116        |Shelli     |Baida     |SBAIDA  |515.127.4563|24-DEC-05|PU_CLERK  |2900  |null          |114       |30           |\n|117        |Sigal      |Tobias    |STOBIAS |515.127.4564|24-JUL-05|PU_CLERK  |2800  |null          |114       |30           |\n|118        |Guy        |Himuro    |GHIMURO |515.127.4565|15-NOV-06|PU_CLERK  |2600  |null          |114       |30           |\n|119        |Karen      |Colmenares|KCOLMENA|515.127.4566|10-AUG-07|PU_CLERK  |2500  |null          |114       |30           |\n+-----------+-----------+----------+--------+------------+---------+----------+------+--------------+----------+-------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6df2e96f-9e35-4afa-98c5-2ef14e77bc77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|Timestamp          |Cleansing Part|Message                                                                                                                                                                                             |\n+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|2024-09-19 14:39:11|Header        |UpperCase in ['employee_id', 'first_name', 'last_name', 'email', 'phone_number', 'hire_date', 'job_id', 'salary', 'commission_pct', 'manager_id', 'department_id'] headers are changed to LowerCase |\n|2024-09-19 14:39:11|Header        |Null header names of indices [] are changed to appropriate name(unnamed_index)                                                                                                                      |\n|2024-09-19 14:39:11|Header        |Headers that are null & data of that column is entirely null are in  indices [] are deleted                                                                                                         |\n|2024-09-19 14:39:24|Data          |Leading and trailing spaces in ['commission_pct', 'manager_id'] columns are removed                                                                                                                 |\n|2024-09-19 14:39:27|Data          |In valid Special_characters in ['commission_pct', 'manager_id'] columns are removed                                                                                                                 |\n|2024-09-19 14:39:27|Data          |Leading and trailing invalid special characters in {} columns are removed and the key(special Character) of dictionary is kept in the leading and trailing according to the {'$': [1, 'salary']}    |\n|2024-09-19 14:39:30|Data          |Missing values in ['commission_pct', 'manager_id'] columns are replaced with null                                                                                                                   |\n|2024-09-19 14:39:35|Data          |['employee_id', 'salary', 'manager_id', 'department_id'] columns are changed from string to integer format                                                                                          |\n|2024-09-19 14:39:35|Data          |['commission_pct'] columns are changed from string to float format                                                                                                                                  |\n|2024-09-19 14:39:36|Data          |Primary Key is valid as it doesn't have any null value                                                                                                                                              |\n|2024-09-19 14:39:36|Data          |Composite Key is valid as they don't have any null value in combination                                                                                                                             |\n|2024-09-19 14:39:37|Data          |0 Duplicates are removed                                                                                                                                                                            |\n+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "d.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4f7545b-987b-4ac3-a6be-c9ad1b20a769",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_datatype(df):\n",
    "    Integer=[]\n",
    "    Float=[]\n",
    "    for i, data_type in df.dtypes:\n",
    "       if data_type == 'string' or isinstance(data_type, StringType):\n",
    "           if df.where(col(i).rlike(\"^[0-9]*.[0-9]*$\")==False).count()==0:\n",
    "               Float.append(i)\n",
    "               continue\n",
    "           if df.where(col(i).rlike(\"^[0-9]*$\")==False).count()==0:\n",
    "               Integer.append(i)\n",
    "               continue\n",
    "    return Integer,Float\n",
    "def change_datatype(df,Integer,Float):\n",
    "    for i in Integer:\n",
    "        df=df.withColumn(i,col(i).cast(\"integer\"))\n",
    "    for i in Float:\n",
    "        df=df.withColumn(i,col(i).cast(\"float\"))\n",
    "    return df\n",
    "\n",
    "    \n",
    "def azure_connection(path,azure_blob):\n",
    "    conf = SparkConf()\n",
    "    conf.set(\"spark.jars.packages\",\n",
    "             \"org.apache.hadoop:hadoop-azure:3.3.5,com.microsoft.azure:azure-storage:8.6.6,com.azure:azure-storage-blob:12.24.0\")\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[1]\") \\\n",
    "        .appName(\"Read CSV\") \\\n",
    "        .config(conf=conf) \\\n",
    "        .config(\"fs.azure.account.key.{}.blob.core.windows.net\".format(azure_blob[\"storage_account_name\"]), azure_blob[\"storage_account_key\"]) \\\n",
    "        .getOrCreate()\n",
    "    df = spark.read.format('csv').option('header', True).load(\"wasbs://{}@{}.blob.core.windows.net/{}\".format(azure_blob[\"container_name\"],azure_blob[\"storage_account_name\"],path))\n",
    "    return df\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2516b212-0745-4365-a3c6-b8d02d2ae3d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|HIRE_DATE|\n+---------+\n|17-JUN-03|\n|21-SEP-05|\n|13-JAN-01|\n|03-JAN-06|\n|21-MAY-07|\n|25-JUN-05|\n|05-FEB-06|\n|07-FEB-07|\n|17-AUG-02|\n|16-AUG-02|\n|28-SEP-05|\n|30-SEP-05|\n|07-MAR-06|\n|07-DEC-07|\n|07-DEC-02|\n|18-MAY-03|\n|24-DEC-05|\n|24-JUL-05|\n|15-NOV-06|\n|10-AUG-07|\n+---------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.select(\"HIRE_DATE\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfa36062-c3b1-4639-8252-32296dd356f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[16]: <bound method DataFrame.describe of DataFrame[employee_id: int, first_name: string, last_name: string, email: string, phone_number: string, hire_date: string, job_id: string, salary: int, commission_pct: float, manager_id: int, department_id: int]>"
     ]
    }
   ],
   "source": [
    "df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe2e5d15-88dd-4123-a916-ef6217efdb9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install streamlit pyngrok -qq -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d8bf8da-acbe-4b21-9dde-15d1c77717b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from curate_data import curate_data  # Ensure this import works\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CurateDataApp\").getOrCreate()\n",
    "\n",
    "st.title('Data Curation App')\n",
    "\n",
    "# Input for data path\n",
    "path = st.text_input('Enter the path to your CSV file:', 'https://gist.githubusercontent.com/kevin336/acbb2271e66c10a5b73aacf82ca82784/raw/e38afe62e088394d61ed30884dd50a6826eee0a8/employees.csv')\n",
    "\n",
    "# Input for primary key\n",
    "primary_key = st.text_input('Enter the primary key column (optional):')\n",
    "\n",
    "# Input for composite keys\n",
    "composite_keys = st.text_input('Enter composite key columns (comma-separated, optional):')\n",
    "composite_keys = [key.strip() for key in composite_keys.split(',')] if composite_keys else None\n",
    "\n",
    "# Input for valid special characters\n",
    "st.subheader('Valid Special Characters')\n",
    "special_char = st.text_input('Enter a special character:')\n",
    "columns = st.text_input('Enter columns for this special character (comma-separated):')\n",
    "if special_char and columns:\n",
    "    valid_spl_chars = {special_char: [1] + [col.strip() for col in columns.split(',')]}\n",
    "else:\n",
    "    valid_spl_chars = None\n",
    "\n",
    "# Button to run the curate_data function\n",
    "if st.button('Curate Data'):\n",
    "    with st.spinner('Curating data...'):\n",
    "        try:\n",
    "            df, logs = curate_data(\n",
    "                path=path,\n",
    "                primary_key=primary_key if primary_key else None,\n",
    "                composite_keys=composite_keys,\n",
    "                valid_spl_chars=valid_spl_chars\n",
    "            )\n",
    "            \n",
    "            st.success('Data curation completed!')\n",
    "            \n",
    "            st.subheader('Curated Data Preview')\n",
    "            st.dataframe(df.limit(10).toPandas())\n",
    "            \n",
    "            st.subheader('Curation Logs')\n",
    "            st.dataframe(logs.toPandas())\n",
    "            \n",
    "            # Display some basic statistics\n",
    "            st.subheader('Data Statistics')\n",
    "            st.write(f\"Total rows: {df.count()}\")\n",
    "            st.write(f\"Total columns: {len(df.columns)}\")\n",
    "            \n",
    "            # Display column names and data types\n",
    "            st.subheader('Column Information')\n",
    "            for col, dtype in df.dtypes:\n",
    "                st.write(f\"{col}: {dtype}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            st.error(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "st.sidebar.info('This app demonstrates the use of the curate_data function.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1523f6c-459f-4796-a722-956084d07415",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Framework 1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
